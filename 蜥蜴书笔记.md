# 机器学习实战 基于Scikit-Learn、Keras和TensorFlow

## CH10 Keras 人工神经网络简介

### 1. 使用Keras实现MLP

#### 1.1. 图像分类器

- 加载数据集

```python
# 加载数据
fashion_mnist = keras.datasets.fashion_mnist
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()

# 创建验证集
X_valid, X_train = X_train_full[: 5000] / 255.0, X_train_full[5000: ] / 255.
y_valid, y_train = y_train_full[: 5000], y_train_full[5000: ]

# 创建标签
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]
```



- 创建模型

```python
# 创建网络
model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape = [28, 28]))
model.add(keras.layers.Dense(300, activation = 'relu'))
model.add(keras.layers.Dense(100, activation = 'relu'))
model.add(keras.layers.Dense(10, activation = 'softmax'))
```

```python
# 模型的相关操作
print('---------------------------------模型信息---------------------------------')
print(model.summary()) # 输出模型信息
#
print('---------------------------------访问特定的层---------------------------------')
print(model.layers[1].name)
#
print('---------------------------------查看各层的参数---------------------------------')
print(model.layers[1].get_weights()) #weights, bias
```



- 编译模型

```python
# 编译模型
model.compile(loss = "sparse_categorical_crossentropy",
              optimizer = "sgd", metrics = ["accuracy"]) 
# 因为不是one-hot编码 如果是one-hot loss用categorical_crossentropy
```

> 训练样本每张图片的ground-truth是一个实数, 因此loss用"sparse_categorical_crossentropy", 如果是one-hot编码的, loss就使用"categorical_crossentropy"
>
> ```python
> keras.utils.to_categorical() # 将非one-hot转化为one-hot
> ```



- 训练和评估模型

  - 训练

  ```python
  histroy = model.fit(X_train, y_train, epochs = 30, validation_data = (X_valid, y_valid))
  
  # 可视化
  pd.DataFrame(histroy.history).plot(figsize = (8, 5))
  plt.grid(True)
  plt.gca().set_ylim(0, 1)
  plt.show()
  ```

  <img src="/Users/dexinghuang/Library/Application Support/typora-user-images/image-20211122211412190.png" alt="image-20211122211412190" style="zoom: 45%;" />

  

  - 评估

  ```python
  model.evaluate(X_test, y_test)
  ```

  

- 使用模型预测

```python
# 使用模型进行预测
X_new = X_test[:3]
y_prob = model.predict(X_new) # 输出属于每一类的概率
print(y_prob.round(2))

y_pred = np.argmax(y_prob, axis = 1) # 输出预测的类别
print(y_pred)
np.array(['Ankle boot', 'Pullover', 'Trouser'], dtype = '<U11')

y_new = y_test[:3] # 真实类别
print(y_new)
```



#### 1.2. 回归MLP

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras

# 载入数据
housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full) # 默认test_size = 0.25

# 预处理
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)

# 模型
model = keras.models.Sequential([
    keras.layers.Dense(30, activation = 'relu', input_shape = X_train.shape[1:]), # 必须加上: (8,)
    keras.layers.Dense(1)])

# 训练
model.compile(loss = "mean_squared_error", optimizer = "sgd")
history = model.fit(X_train, y_train, epochs = 20, validation_data = (X_valid, y_valid))

# 测试
mse_test = model.evaluate(X_test, y_test)
X_new = X_test[:3]
y_pred = model.predict(X_new)
```



#### 1.3. 构造复杂模型

- 宽神经网络

<img src="/Users/dexinghuang/Library/Application Support/typora-user-images/image-20211122214228746.png" alt="image-20211122214228746" style="zoom:25%;" />

```python
input_ = keras.layers.Input(shape = X_train.shape[1:])
hidden1 = keras.layers.Dense(30, activation = "relu")(input_)
hidden2 = keras.layers.Dense(30, activation = "relu")(hidden1)
concat = keras.layers.Concatenate()([input_, hidden2])
output = keras.layers.Dense(1)(concat)
model = keras.Model(inputs = [input_], outputs = [output])
```



- 多个输入

<img src="/Users/dexinghuang/Library/Application Support/typora-user-images/image-20211122214756835.png" alt="image-20211122214756835" style="zoom:20%;" />

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras

# 多个输入
housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full) # 默认test_size = 0.25

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)

# 划分两个输入
X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]
X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]
X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]

# 构建网络
input_A = keras.layers.Input(shape = [5], name = "wide_input")
input_B = keras.layers.Input(shape = [6], name = "deep_input")
hidden1 = keras.layers.Dense(30, activation = "relu")(input_B)
hidden2 = keras.layers.Dense(30, activation = "relu")(hidden1)
concat = keras.layers.concatenate([input_A, input_B])
output = keras.layers.Dense(1, name = "output")(concat)
model = keras.Model(inputs = [input_A, input_B], outputs = [output])

# 训练
model.compile(loss = "mean_squared_error", optimizer = "sgd")

history = model.fit((X_train_A, X_train_B), y_train, epochs = 20, validation_data = ((X_valid_A, X_valid_B), y_valid))
mse_test = model.evaluate((X_test_A, X_test_B), y_test)
```



- 多个输出

<img src="/Users/dexinghuang/Library/Application Support/typora-user-images/image-20211123144133307.png" alt="image-20211123144133307" style="zoom:25%;" />

> 主要部分与多输出相同, 但在fit的过程中需要为每个输出添加损失函数

```python
# 构建网络
input_A = keras.layers.Input(shape = [5], name = "wide_input")
input_B = keras.layers.Input(shape = [6], name = "deep_input")
hidden1 = keras.layers.Dense(30, activation = "relu")(input_B)
hidden2 = keras.layers.Dense(30, activation = "relu")(hidden1)
concat = keras.layers.concatenate([input_A, input_B])
output = keras.layers.Dense(1, name = "main_output")(concat)
aux_output = keras.layers.Dense(11, name = "aux_output")(hidden2)
model = keras.Model(inputs = [input_A, input_B], output = [output, aux_output])

# 训练
model.compile(loss = ["mse", "mse"], loss_weights = [0.9, 0.1], optimizer = keras.optimizers.SGD(learning_rate=1e-3))
# 一般是构成一个联合的损失函数训练 还可以分配损失函数的权重
history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs = 20, validation_data = ([X_valid_A, X_valid_B], [y_valid, y_valid]))
total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])
```



- 使用Model子类API构建动态模型

**实现的网络模型跟上面的一样**

```python
class WideAndDeepModel(keras.Model):
    def __init__(self, units=30, activation="relu", **kwargs):
        super().__init__(**kwargs)
        self.hidden1 = keras.layers.Dense(units, activation=activation)
        self.hidden2 = keras.layers.Dense(units, activation=activation)
        self.main_output = keras.layers.Dense(1)
        self.aux_output = keras.layers.Dense(1)

    def call(self, inputs):
        input_A, input_B = inputs
        hidden1 = self.hidden1(input_B)
        hidden2 = self.hidden2(hidden1)
        concat = keras.layers.concatenate([input_A, hidden2])
        main_output = self.main_output(concat)
        aux_output = self.aux_output(hidden2)
        return main_output, aux_output
```



#### 1.4. 保存和还原模型

> 使用顺序式API或者函数式API保存 / 加载训练好的模型十分简单

- 保存

```python
model = keras.models.Sequential([...])
model.compile([...])
model.fit([...])
model.save("my_keras_model.h5")
# 保存每一层模型的参数和优化器
```



- 加载

```python
model = keras.models.load_model("my_keras.model.h5")
```



#### 1.5. 回调函数

> 在训练几个小时的模型时, 保存特定时间节点的数据, 避免计算机崩溃时丢失数据.

- ModelCheckpoint (定期保存检查点)

```python
checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.h5", save_best_only=True)
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid),callbacks=[checkpoint_cb])
model = keras.models.load_model("my_keras_model.h5")
```

> 如果训练过程中包括验证集, 那么可以设置save_best_only = True, 只有在验证集上表现最好的才保存



- EarlyStopping(提前停止训练)

> 如果在多个epoch上验证集正确率没有进展, 那么可以中断训练, 回到最佳的模型

```
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb, early_stopping_cb])
```



- 自定义回调

```python
class PrintValTrainRatioCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        print("\nval/train: {:.2f}".format(logs["val_loss"] / logs["loss"]))

val_train_ratio_cb = PrintValTrainRatioCallback()
history = model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid), callbacks=[val_train_ratio_cb])
```



### 2. 微调神经网络超参数

> 神经网络中的超参数有许多, 需要进行适当的调整以达到最佳的性能, 可以使用sklearn中的**GridSearchCV**或者**RandomizedSearchCV**来搜索, 需要将keras模型包装一下

```python
from tensorflow import keras
import numpy as np
from scipy.stats import reciprocal
from sklearn.model_selection import RandomizedSearchCV
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

def build_model(n_hidden = 1, n_neurons = 30, learning_rate = 3e-3, input_shape = [8]):
    model = keras.models.Sequential()
    model.add(keras.layers.InputLayer(input_shape = input_shape))
    for layer in range(n_hidden):
        model.add(keras.layers.Dense(n_neurons, activation = 'relu'))
    model.add(keras.layers.Dense(1))
    optimizer = keras.optimizers.SGD(lr = learning_rate)
    model.compile(loss = "mse", optimizer = optimizer)
    return model

housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full) # 默认test_size = 0.25

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)

keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)

# 参数搜索范围设置
param_distribs = {
    "n_hidden": [0, 1, 2, 3],
    "n_neurons": np.arange(1, 100),
    "learning_rate": reciprocal(3e-4, 3e-2)
}

rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter = 10, cv = 3)
rnd_search_cv.fit(X_train, y_train, epochs = 100,
                  validation_data = (X_valid, y_valid),
                  callbacks = [keras.callbacks.EarlyStopping(patience = 10)])

print(rnd_search_cv.best_params_)

model = rnd_search_cv.best_estimator_.model
# {'learning_rate': 0.01106215283919978, 'n_hidden': 3, 'n_neurons': 13}
```



## CH 11 训练深度神经网络

​		训练深层的神经网络会遇到许多问题, 首先就是**梯度消失/爆炸**, 因为网络更新基于的是反向传播, 越深的层梯度值越小. 其次, 越深的网络需要的数据也越多, 否则会欠拟合, 但有时候并没有这么多数据, **迁移学习**的出现较好的解决了这一问题. 网络深, 参数多训练也会变慢, 因此也出现了许多**不同的优化器**(之前用的一般是sgd). 最后就是过拟合的问题, 通过各种**正则化**方式来减小过拟合的风险.



### 1. 梯度消失/爆炸问题

- 合适的初始化

> 主要思想就是保证每层输入的方差等于输出的方差, 使用的策略是**Xavier初始化**

<img src="/Users/dexinghuang/Library/Application Support/typora-user-images/image-20211123231359186.png" alt="image-20211123231359186" style="zoom: 50%;" />

```python
keras.layers.Dense(10, activation="relu", kernel_initializer="he_normal")
# fan_{avg}
init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',
                                          distribution='uniform')
keras.layers.Dense(10, activation="relu", kernel_initializer=init)
```



- 激活函数的选取

> ReLU激活函数在小于0时恒为0, 采用ReLU的变体, 不过现在还是采用ReLu比较多, 就不细说



- BatchNormalization

> 适合的初始化可以显著减少在训练开始时的梯度消失/爆炸问题, 但是在训练过程中无法保证, 因此提出了一种BN算法来解决这一问题, 通过在每个隐藏层的激活函数之前或之后增加一个操作, 对**每个输入零中心并归一化**.
>
> > 批量归一化算法
> >
> > $\mu_{B}=\frac{1}{m_B}\sum_{i=1}^{m_B}x^{(i)}$
> >
> > $\sigma^{2}_B=\frac{1}{m_B}\sum_{i=1}^{m_B}(x^{(i)}-\mu_B)^2$
> >
> > $\hat{x}^{(i)}=\frac{x^{(i)}-\mu_B}{\sqrt{\sigma_{B}^2+\epsilon}}$
> >
> > $z^{(i)}=\gamma\otimes \hat{x}^{(i)}+\beta$

> BN增加了模型的复杂度$\mu,\sigma,\epsilon,\beta$, 前两个是不可训练的参数, 同时BN作者主张**在激活函数之前加入BN**

```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(300, use_bias=False),
    keras.layers.BatchNormalization(),
    keras.layers.Activation("relu"),
    keras.layers.Dense(100, use_bias=False),
    keras.layers.BatchNormalization(),
    keras.layers.Activation("relu"),
    keras.layers.Dense(10, activation="softmax")
])
```



- 梯度裁剪

> 梯度裁剪将梯度限制在某个阈值内, 经常用于循环神经网络, 因为在RNN中使用BN有点困难.

```python
optimizer = keras.optimizers.SGD(clipvalue=1.0) # [0.9, 100] -> [0.9, 1]
optimizer = keras.optimizers.SGD(clipnorm=1.0) # 不改变梯度方向 [0.9, 100] -> [0.008, 0.999]

model.compile(loss = "mse", optimizer = optimizer)
```



### 2. Pre-train(加快训练速度, 减少数据集用量)

- 迁移学习

```python
from tensorflow import keras
import numpy as np
import tensorflow as tf

def split_dataset(X, y):
    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts
    y_A = y[~y_5_or_6]
    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7
    y_B = (y[y_5_or_6] == 6).astype(np.float32) # 二分类问题
    return ((X[~y_5_or_6], y_A), (X[y_5_or_6], y_B))

# 数据准备
(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()
X_train_full = X_train_full / 255.0
X_test = X_test / 255.0
X_valid, X_train = X_train_full[:5000], X_train_full[5000:]
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)
(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)
(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)
# 小数据集
X_train_B = X_train_B[:200]
y_train_B = y_train_B[:200]

tf.random.set_seed(42)
np.random.seed(42)

# 大模型
model_A = keras.models.Sequential()
model_A.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_hidden in (300, 100, 50, 50, 50):
    model_A.add(keras.layers.Dense(n_hidden, activation="selu"))
model_A.add(keras.layers.Dense(8, activation="softmax"))

model_A.compile(loss=keras.losses.sparse_categorical_crossentropy,
                optimizer=keras.optimizers.SGD(learning_rate=1e-3),
                metrics=["accuracy"])

history = model_A.fit(X_train_A, y_train_A, epochs=20,
                    validation_data=(X_valid_A, y_valid_A))

model_A.save("my_model_A.h5")

# 在小数据集上不使用迁移学习, 直接训练
model_B = keras.models.Sequential()
model_B.add(keras.layers.Flatten(input_shape=[28, 28]))
for n_hidden in (300, 100, 50, 50, 50):
    model_B.add(keras.layers.Dense(n_hidden, activation="selu"))
model_B.add(keras.layers.Dense(1, activation="sigmoid"))

model_B.compile(loss=keras.losses.binary_crossentropy,
                optimizer=keras.optimizers.SGD(learning_rate=1e-3),
                metrics=["accuracy"])

history = model_B.fit(X_train_B, y_train_B, epochs=20,
                      validation_data=(X_valid_B, y_valid_B))

# 迁移学习
model_A = keras.models.load_model("my_model_A.h5")
model_A_clone = keras.models.clone_model(model_A)
model_A_clone.set_weights(model_A.get_weights())
model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])
model_B_on_A.add(keras.layers.Dense(1, activation="sigmoid"))

# 冻结
for layer in model_B_on_A.layers[:-1]:
    layer.trainable = False

model_B_on_A.compile(loss=keras.losses.binary_crossentropy,
                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),
                     metrics=["accuracy"])

history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,
                           validation_data=(X_valid_B, y_valid_B))

# 解冻微调, 使用较小学习率
for layer in model_B_on_A.layers[:-1]:
    layer.trainable = True

model_B_on_A.compile(loss=keras.losses.binary_crossentropy,
                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),
                     metrics=["accuracy"])
history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,
                           validation_data=(X_valid_B, y_valid_B))

# 在测试集表现
model_B.evaluate(X_test_B, y_test_B)
model_B_on_A.evaluate(X_test_B, y_test_B)
```

> 在MLP中迁移学习的作用体现的并不明显, 在CNN中会进一步介绍



### 3. 优化器(加速网络收敛)

- 动量优化

> 关心先前的梯度, 产生加速度

```python
optimizer = keras.optimizers.SGD(lr=0.001, momentum = 0.9)
```



- Nesterov加速梯度

> 在动量上的优化

```python
optimizer = keras.optimizers.SGD(lr=0.001, momentum = 0.9, nesterov = True)
```



- AdaGrad(不怎么用)
- RMSProp

```
optimizer = keras.optimizers.RMSprop(lr=0.001, rho = 0.9)
```



- Adam和Nadam

```python
optimizer = keras.optimizers.Adam(lr=0.001, beta_1 = 0.9, beta_2 = 0.99)
```



- 自适应**学习率**

> - 幂调度
>
> ```python
> optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)
> ```
>
> 
>
> - 指数调度 / 分段调度
>
> ```python
> def exponential_decay(lr0, s):
>     def exponential_decay_fn(epoch):
>         return lr0 * 0.1**(epoch / s)
>     return exponential_decay_fn
> 
> exponential_decay_fn = exponential_decay(lr0=0.01, s=20)
> 
> lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)
> history = model.fit(X_train_scaled, y_train, epochs=n_epochs,
>                     validation_data=(X_valid_scaled, y_valid),
>                     callbacks=[lr_scheduler])
> ```
>
> 
>
> - 分段恒定调度
>
> ```python
> def piecewise_constant(boundaries, values):
>     boundaries = np.array([0] + boundaries)
>     values = np.array(values)
>     def piecewise_constant_fn(epoch):
>         return values[np.argmax(boundaries > epoch) - 1]
>     return piecewise_constant_fn
> 
> lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)
> ```
>
> 
>
> - 性能调度
> - 1周期调度



### 4. 正则化(防止过拟合)

- $\mathcal{l}_1,l_2$正则化

```python
layer = keras.layers.Dense(100, activation="elu",
                           kernel_initializer="he_normal",
                           kernel_regularizer=keras.regularizers.l2(0.01))
```



- dropout

```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(300, activation="elu", kernel_initializer="he_normal"),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(100, activation="elu", kernel_initializer="he_normal"),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(10, activation="softmax")
])
```



- 最大范数正则化

```python
layer = keras.layers.Dense(100, activation="selu", kernel_initializer="lecun_normal",
                           kernel_constraint=keras.constraints.max_norm(1.))
```

